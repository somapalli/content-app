Welcome to our Content Display App!

This is a sample content that can be dynamically injected into the HTML file.

Features:
- Dynamic content loading
- Responsive design
- Easy to update

You can add more content by:
1. Editing this text file
2. Using the "Add Content" button
3. Updating the content through the API

Feel free to customize this content according to your needs! 

Got it üëç ‚Äî thanks for the clarification.

Below is the **complete, clean Architecture & Performance (SLA) document**, **ready to paste into Confluence / Word**, with:

* ‚úÖ **ONLY the calculation section improved for clarity**
* ‚úÖ **All other content kept exactly as original**
* ‚ùå **No ‚Äúunchanged‚Äù, ‚Äúkeep same‚Äù, or editorial notes**

---

# Daily File Ingestion ‚Üí Transactions ‚Üí Pricing/Fee ‚Üí Ledger

## Architecture & Performance / SLA Document (MVP 1.2)

---

## 1) Purpose

Build a daily batch processing pipeline that ingests a file once per day, processes ~2,000,000 records end-to-end within a 4‚Äì5 hour SLA, calculates pricing/fees (via an external fee service), persists results to transaction storage, and posts to ledger via a separate decoupled process.

---

## 2) Scope

### In scope

* Daily file intake + validation
* Kafka ingestion (topic)
* Sequential record processing (with controlled parallelism allowed)
* Intermediate tables persistence
* Transaction tables persistence
* External fee/pricing calculation integration
* Persist pricing details to transaction tables
* Ledger posting as a decoupled process (async / separate workflow)
* SLA measurement, dashboards, alerts, and runbook

### Out of scope (for MVP 1.2 unless explicitly added)

* Real-time streaming continuous ingestion
* Complex reconciliation automation beyond counts/status dashboards
* Multiple file formats and multiple files/day (unless required)

---

## 3) High-Level Architecture

### 3.1 Logical Components

**File Intake Service**

* Reads file once per day
* Validates schema, header/trailer counts, checksum (optional)
* Emits records to Kafka with correlation keys

**Kafka / Event Streaming**

* Topic for raw records (e.g., daily-file-raw)
* Consumer group for processing pipeline
* DLQ topic for bad records (e.g., daily-file-dlq)
* Retry topic(s) optional

**Processing Orchestrator (Consumer / Worker)**

* Pulls from Kafka
* Applies control flow: validate ‚Üí persist intermediate ‚Üí persist transaction ‚Üí call fee service ‚Üí persist pricing
* Manages ordering/commit strategy (sequential commit, controlled concurrency)

**Data Persistence Layer**

* Intermediate tables: store near-source representation (minimal transformation)
* Transaction tables: normalized/enriched transaction model
* Pricing columns/tables: fees, adjustments, rate plan metadata, calculated amounts

**External Fee/Pricing Service**

* Stateless API preferred
* Idempotent requests required
* Caching allowed/encouraged (if pricing rules stable during window)

**Ledger Posting Process (Decoupled)**

* Triggered off ‚Äútransaction priced‚Äù events or database change feed
* Posts to ledger record-by-record
* Retries, idempotency keys, and reconciliation support

**Observability & Control Plane**

* Metrics, logs, traces
* SLA dashboards (end-to-end + per-stage)
* Alerting (lag, latency, error rate, slowdowns)
* Operational controls: pause/resume, re-drive DLQ, replay by file id/date

---

## 4) Data Flow

### 4.1 End-to-End Flow (Narrative)

File read (once/day) ‚Üí records produced to Kafka topic

Kafka consumer processes records:

* Insert into intermediate tables
* Insert into transaction tables
* Call external fee service to compute fees
* Persist fees/pricing into transaction pricing storage

Ledger process runs separately:

* Consumes ‚Äúpriced transaction‚Äù events (or polls table)
* Calls ledger service record-by-record

### 4.2 Mermaid Diagram

```mermaid
flowchart LR
  A[Daily File] --> B[File Intake Service]
  B --> C[(Kafka Topic: daily-file-raw)]
  C --> D[Processor / Consumer Group]

  D --> E[(Intermediate Tables)]
  D --> F[(Transaction Tables)]
  D --> G[External Fee Service]
  G --> D
  D --> H[(Transaction Pricing Storage)]

  H --> I[(Kafka/Event: transaction-priced)]
  I --> J[Ledger Posting Worker]
  J --> K[Ledger System]

  D --> L[(DLQ Topic)]
  J --> M[(Ledger DLQ)]
```

---

## 5) Performance Targets & Throughput Math

### 5.1 Input Volumes & Time Window

* Total volume: 2,000,000 records/day
* Batch window: 4‚Äì5 hours
* Target completion: 4.5 hours design point (buffer for spikes/retries)

4.5 hours = 4.5 √ó 60 √ó 60 = **16,200 seconds**

---

### 5.2 Required Minimum Throughput

[
\text{Required Throughput} = \frac{2,000,000}{16,200} \approx 123 \text{ records/sec}
]

This represents the absolute minimum sustained throughput assuming no retries, no pauses, and no downstream delays.

---

### 5.3 Design Throughput Target (With Headroom)

To safely meet SLA under real-world conditions, the system must be designed with headroom.

* **Design throughput target:** **150‚Äì175 records/sec**

This provides ~20‚Äì40% buffer above the minimum requirement.

| Throughput | Approximate Completion Time |
| ---------- | --------------------------- |
| 123 rps    | ~4.5 hours                  |
| 150 rps    | ~3.7 hours                  |
| 175 rps    | ~3.2 hours                  |

This buffer absorbs:

* External service latency variation
* Retry overhead
* Kafka rebalances
* Database contention and GC pauses

---

### 5.4 Sequential Processing with Controlled Parallelism

Even if business requirements state ‚Äúsequential record processing,‚Äù true single-threaded execution will not meet SLA at this scale.

Recommended approach:

* Fetch records from Kafka in batches (e.g., 500‚Äì2,000)
* Process records concurrently using bounded concurrency (e.g., 10‚Äì50 workers)
* Commit offsets in order once all prior records are completed

This preserves ordering guarantees while achieving required throughput.

---

### 5.5 External Fee Service Capacity Implication

At 150‚Äì175 records/sec end-to-end throughput:

* Fee service must support sustained parallel requests during the batch window

Example:

* Target throughput: 150 rps
* Fee service p95 latency: 75 ms

[
\text{Required Concurrency} \approx 150 \times 0.075 \approx 12
]

Recommended provisioning: **20‚Äì30 concurrent fee service calls** to absorb latency spikes and retries.

---

## 6) SLA Model: End-to-End + Per Layer

### 6.1 End-to-End SLA Definition

SLA-E2E: From ‚Äúfile intake start‚Äù ‚Üí ‚Äúlast record persisted with pricing‚Äù

Target: ‚â§ 5 hours (goal ‚â§ 4.5 hours)

Success criteria:

* ‚â• 99.5% records completed within window
* Remainder handled via retry/DLQ re-drive within extended operational window

---

### 6.2 Per-Layer SLAs

Each stage must report:

* Throughput (records/sec)
* Latency (p50/p95/p99)
* Error rate
* Queue depth / lag
* Retry counts

Suggested time budget:

| Stage                               | Time Budget    |
| ----------------------------------- | -------------- |
| File intake + publish to Kafka      | 30‚Äì45 min      |
| Kafka ‚Üí Intermediate tables         | 60 min         |
| Intermediate ‚Üí Transaction tables   | 60 min         |
| External fee calc + pricing persist | 60‚Äì75 min      |
| Buffer for retries/slowdowns        | 30‚Äì45 min      |
| **Total**                           | **~4‚Äì5 hours** |

---

## 7) External Fee Service SLA Requirements

### 7.1 Required Effective Throughput

* Sustained ‚â• 175 records/sec during batch window

### 7.2 Latency SLA

| Metric       | Required SLA |
| ------------ | ------------ |
| p50 latency  | ‚â§ 30 ms      |
| p95 latency  | ‚â§ 75 ms      |
| p99 latency  | ‚â§ 120 ms     |
| Availability | ‚â• 99.9%      |
| Error rate   | < 0.1%       |
| Timeout      | 150‚Äì250 ms   |
| Idempotency  | Mandatory    |

---

## 8) Ledger Posting Architecture (Decoupled)

### 8.1 Ledger SLA

Ledger posting must not block the main batch SLA.

Target: ‚â§ 1‚Äì2 hours after pricing completes (or within same business day)

---

## 9) Data Design (Minimum)

### 9.1 Intermediate Tables

Purpose: auditability, troubleshooting, replay support

Fields: file_id, record_seq, raw_payload_hash, parsed fields, ingest_timestamp, status

### 9.2 Transaction Tables

Canonical transaction entity including correlation keys, event types, amounts, merchant hierarchy keys, timestamps, statuses

### 9.3 Pricing Storage

Hybrid approach:

* transaction_pricing table for fee lines
* summary pricing columns on transaction table

---

## 10) Scalability & Capacity Planning

* Target throughput: 150‚Äì175 records/sec
* Kafka partitions: 12‚Äì24
* Batch DB inserts with chunked commits
* Avoid per-record DB transactions

---

## 11) Observability

Metrics include:

* Throughput, latency, error rates
* Kafka lag and DLQ depth
* External fee service latency and errors
* Ledger backlog and retries

---

## 12) Reliability, Retries, and DLQ Strategy

* Retry transient failures with backoff
* Route exhausted retries to DLQ
* Support replay by file_id/date
* Enforce idempotency at every stage

---

## 13) Performance Testing Plan

* Baseline load: 2M records
* Degraded fee service scenarios
* DB contention tests
* Failure recovery and replay

Acceptance:

* End-to-end ‚â§ 5 hours
* Sustained ‚â• 150 rps
* DLQ rate < 0.1%

---

## 14) Operational Runbook

* Start batch
* Monitor ETA, lag, latency
* Pause/resume consumers
* Enable fallback pricing if required
* Re-drive DLQ
* Validate counts and ledger backlog

---

## 15) Key Risks & Mitigations

| Risk                         | Impact          | Mitigation                             |
| ---------------------------- | --------------- | -------------------------------------- |
| External fee latency         | SLA miss        | Concurrency + caching + async fallback |
| DB contention                | Slow processing | Batch commits, tuning                  |
| Strict sequential processing | Throughput cap  | Ordered commit + parallelism           |
| Ledger blocking              | SLA miss        | Decoupled async ledger                 |
| Duplicate reprocessing       | Data issues     | Idempotent keys                        |

---

## Appendix A ‚Äî Sequential but Fast Pattern

Consume in batches, process concurrently, and commit offsets in order to preserve correctness while achieving required throughput.


